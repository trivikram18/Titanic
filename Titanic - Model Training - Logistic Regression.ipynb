{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Dataset - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ReadIn the training data\n",
    "titanic_train = pd.read_csv(\"titanic_train.csv\")\n",
    "print (titanic_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">pclass: A proxy for socio-economic status (SES)\n",
    "1st = Upper\n",
    "2nd = Middle\n",
    "3rd = Lower\n",
    "\n",
    ">age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
    "\n",
    ">sibsp: The dataset defines family relations in this way...\n",
    "Sibling = brother, sister, stepbrother, stepsister\n",
    "Spouse = husband, wife (mistresses and fiancÃ©s were ignored)\n",
    "\n",
    ">parch: The dataset defines family relations in this way...\n",
    "Parent = mother, father\n",
    "Child = daughter, son, stepdaughter, stepson\n",
    "Some children travelled only with a nanny, therefore parch=0 for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate out the target/ label from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All column names: Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n",
      "Training data column names: Index(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n",
      "       'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n",
      "Training label: Survived\n"
     ]
    }
   ],
   "source": [
    "print(f'All column names: {titanic_train.columns}')\n",
    "X_train = titanic_train.copy()\n",
    "y_train = X_train.pop('Survived')\n",
    "print(f'Training data column names: {X_train.columns}')\n",
    "print(f'Training label: {y_train.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "c3b88b378d4108c61a0f769befb334946f3dbf1d"
   },
   "outputs": [],
   "source": [
    "def substrings_in_string(big_string, substrings):\n",
    "    for substring in substrings:\n",
    "        if str.find(big_string, substring) != -1:\n",
    "            return substring\n",
    "    # print (big_string)\n",
    "    return np.nan\n",
    "\n",
    "def replace_titles(x):\n",
    "    title=x['salut']\n",
    "    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col', 'Sir']:\n",
    "        return 'Mr'\n",
    "    elif title in ['the Countess', 'Mme', 'Lady', 'Dona']:\n",
    "        return 'Mrs'\n",
    "    elif title in ['Mlle', 'Ms']:\n",
    "        return 'Miss'\n",
    "    elif title =='Dr':\n",
    "        if x['Sex']=='Male':\n",
    "            return 'Mr'\n",
    "        else:\n",
    "            return 'Mrs'\n",
    "    else:\n",
    "        return title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing - Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values from salut - training dataset:\n",
      " ['Mr' 'Mrs' 'Miss' 'Master' 'Don' 'Rev' 'Dr' 'Mme' 'Ms' 'Major' 'Lady'\n",
      " 'Sir' 'Mlle' 'Col' 'Capt' 'the Countess' 'Jonkheer'] \n",
      "\n",
      "salut Before:\n",
      "Mr              517\n",
      "Miss            182\n",
      "Mrs             125\n",
      "Master           40\n",
      "Dr                7\n",
      "Rev               6\n",
      "Mlle              2\n",
      "Col               2\n",
      "Major             2\n",
      "Sir               1\n",
      "Ms                1\n",
      "Don               1\n",
      "the Countess      1\n",
      "Lady              1\n",
      "Jonkheer          1\n",
      "Capt              1\n",
      "Mme               1\n",
      "Name: salut, dtype: int64 \n",
      "\n",
      "Index(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n",
      "       'Ticket', 'Fare', 'Cabin', 'Embarked', 'salut'],\n",
      "      dtype='object') \n",
      "\n",
      "salut After:\n",
      "Mr        531\n",
      "Miss      185\n",
      "Mrs       135\n",
      "Master     40\n",
      "Name: salut, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>salut</th>\n",
       "      <th>Master</th>\n",
       "      <th>Miss</th>\n",
       "      <th>Mr</th>\n",
       "      <th>Mrs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63.0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70.5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "salut  Master  Miss  Mr  Mrs\n",
       "Age                         \n",
       "62.0        0     0   3    1\n",
       "63.0        0     1   0    1\n",
       "64.0        0     0   2    0\n",
       "65.0        0     0   3    0\n",
       "66.0        0     0   1    0\n",
       "70.0        0     0   2    0\n",
       "70.5        0     0   1    0\n",
       "71.0        0     0   2    0\n",
       "74.0        0     0   1    0\n",
       "80.0        0     0   1    0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split Name and extract the salutation\n",
    "\n",
    "X_train['salut'] = X_train['Name'].str.split(',',expand=True)[1].str.split('.',expand=True)[0].str.strip()\n",
    "print(\"Unique values from salut - training dataset:\\n\", X_train['salut'].unique(), \"\\n\")\n",
    "\n",
    "print (\"salut Before:\")\n",
    "print (X_train['salut'].value_counts(), \"\\n\")\n",
    "\n",
    "# X_train.drop(['firstname', 'last_name', 'lastname', 'lastname1'], axis=1, inplace=True)\n",
    "print (X_train.columns, \"\\n\")\n",
    "\n",
    "X_train['salut']=X_train.apply(replace_titles, axis=1)\n",
    "print (\"salut After:\")\n",
    "print (X_train['salut'].value_counts())\n",
    "\n",
    "Age_salut = pd.crosstab(X_train.Age, X_train.salut)\n",
    "Age_salut.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing data to fill in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values for Age before imputation:  177\n",
      "Null values for Age after imputation:  0\n"
     ]
    }
   ],
   "source": [
    "# Imputing Age - We are using the 'salut' feature to group the respondent to impute the age\n",
    "print (\"Null values for Age before imputation: \", X_train['Age'].isnull().sum())\n",
    "X_train['Age'] = X_train.groupby('salut').Age.transform(lambda x: x.fillna(x.mean()))\n",
    "print (\"Null values for Age after imputation: \", X_train['Age'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values for Cabin before imputation:  687\n",
      "Value Counts of Cabin - Before\n",
      "NaN            687\n",
      "B96 B98          4\n",
      "G6               4\n",
      "C23 C25 C27      4\n",
      "C22 C26          3\n",
      "              ... \n",
      "E77              1\n",
      "B71              1\n",
      "D47              1\n",
      "C91              1\n",
      "E63              1\n",
      "Name: Cabin, Length: 148, dtype: int64\n",
      "Value Counts of Cabin - After\n",
      "Null           687\n",
      "B96 B98          4\n",
      "C23 C25 C27      4\n",
      "G6               4\n",
      "D                3\n",
      "              ... \n",
      "C148             1\n",
      "D28              1\n",
      "C70              1\n",
      "E10              1\n",
      "E49              1\n",
      "Name: Cabin, Length: 148, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Imputing Cabin - This cannot be imputed as there is no logic and hence we fill the NAs with 'Null' string\n",
    "print(\"Null values for Cabin before imputation: \", X_train['Cabin'].isnull().sum())\n",
    "\n",
    "print(\"Value Counts of Cabin - Before\")\n",
    "print (X_train['Cabin'].value_counts(dropna = False))\n",
    "\n",
    "X_train['Cabin'] = X_train['Cabin'].fillna('Null')\n",
    "\n",
    "print(\"Value Counts of Cabin - After\")\n",
    "print (X_train['Cabin'].value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values after imputation: \n",
      "PassengerId    0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Cabin          0\n",
      "Embarked       0\n",
      "salut          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Imputing the whole dataset just in case there are any furhter missing values\n",
    "X_train = X_train.fillna(method='ffill').fillna(method='bfill')\n",
    "print(\"Null values after imputation: \")\n",
    "print(X_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Null    687\n",
       "C        59\n",
       "B        47\n",
       "D        33\n",
       "E        33\n",
       "A        15\n",
       "F        12\n",
       "G         4\n",
       "T         1\n",
       "Name: Deck, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deck\n",
    "cabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G', 'Null']\n",
    "X_train['Deck']=X_train['Cabin'].map(lambda x: substrings_in_string(x, cabin_list))\n",
    "\n",
    "X_train['Deck'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     537\n",
      "2     161\n",
      "3     102\n",
      "4      29\n",
      "6      22\n",
      "5      15\n",
      "7      12\n",
      "11      7\n",
      "8       6\n",
      "Name: FamilySize, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Family Size and Fare per Passenger\n",
    "X_train['FamilySize'] = X_train['SibSp'] + X_train['Parch'] + 1\n",
    "# X_train['FarePerPassenger'] = X_train['Fare']/(X_train['FamilySize'])\n",
    "\n",
    "print(X_train['FamilySize'].value_counts())\n",
    "# print()\n",
    "# print(X_train['FarePerPassenger'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.to_csv(\"Titanic_Train_Processed.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Categorical and String features into Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "eab93f8f93e80eda4dc0cb408eeb64e2b5ae98b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training dataset after One Hot Encoding:  (891, 855)\n",
      "    Age  SibSp  Parch     Fare  FamilySize  Pclass_1  Pclass_2  Pclass_3  \\\n",
      "0  22.0      1      0   7.2500           2         0         0         1   \n",
      "1  38.0      1      0  71.2833           2         1         0         0   \n",
      "2  26.0      0      0   7.9250           1         0         0         1   \n",
      "3  35.0      1      0  53.1000           2         1         0         0   \n",
      "4  35.0      0      0   8.0500           1         0         0         1   \n",
      "\n",
      "   Sex_female  Sex_male  ...  Ticket_WE/P 5735  Deck_A  Deck_B  Deck_C  \\\n",
      "0           0         1  ...                 0       0       0       0   \n",
      "1           1         0  ...                 0       0       0       1   \n",
      "2           1         0  ...                 0       0       0       0   \n",
      "3           1         0  ...                 0       0       0       1   \n",
      "4           0         1  ...                 0       0       0       0   \n",
      "\n",
      "   Deck_D  Deck_E  Deck_F  Deck_G  Deck_Null  Deck_T  \n",
      "0       0       0       0       0          1       0  \n",
      "1       0       0       0       0          0       0  \n",
      "2       0       0       0       0          1       0  \n",
      "3       0       0       0       0          0       0  \n",
      "4       0       0       0       0          1       0  \n",
      "\n",
      "[5 rows x 855 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop features which are unique across respondents as they are not useful\n",
    "X_train.drop(['Name', 'PassengerId'], axis=1, inplace=True)\n",
    "\n",
    "# One Hot Encoding - To convert categorical to binary data\n",
    "X_train_dummies = pd.get_dummies(X_train, columns=['Pclass', 'Sex', 'Cabin', 'Embarked', 'salut', 'Ticket', 'Deck'])\n",
    "\n",
    "print (\"Shape of training dataset after One Hot Encoding: \", X_train_dummies.shape)\n",
    "print (X_train_dummies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data - ReadIn, Preprocess, Imputing, Feature Engineering and One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReadIn the test data\n",
    "\n",
    "titanic_test = pd.read_csv(\"titanic_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age             86\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             1\n",
       "Cabin          327\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values from salut - test dataset:\n",
      " ['Mr' 'Mrs' 'Miss' 'Master' 'Ms' 'Col' 'Rev' 'Dr' 'Dona']\n",
      "salut Before:\n",
      "Mr        240\n",
      "Miss       78\n",
      "Mrs        72\n",
      "Master     21\n",
      "Col         2\n",
      "Rev         2\n",
      "Dona        1\n",
      "Ms          1\n",
      "Dr          1\n",
      "Name: salut, dtype: int64\n",
      "Index(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n",
      "       'Ticket', 'Fare', 'Cabin', 'Embarked', 'salut'],\n",
      "      dtype='object')\n",
      "salut After:\n",
      "Mr        244\n",
      "Miss       79\n",
      "Mrs        74\n",
      "Master     21\n",
      "Name: salut, dtype: int64\n",
      "salut  Master  Miss  Mr  Mrs\n",
      "Age                         \n",
      "0.17        0     1   0    0\n",
      "0.33        1     0   0    0\n",
      "0.75        1     0   0    0\n",
      "0.83        1     0   0    0\n",
      "0.92        0     1   0    0\n",
      "1.00        0     3   0    0\n",
      "salut  Master  Miss  Mr  Mrs\n",
      "Age                         \n",
      "61.0        0     0   2    0\n",
      "62.0        0     0   1    0\n",
      "63.0        0     0   1    1\n",
      "64.0        0     0   1    2\n",
      "67.0        0     0   1    0\n",
      "76.0        0     0   0    1\n"
     ]
    }
   ],
   "source": [
    "X_test = titanic_test.copy()\n",
    "\n",
    "# Split Name and extract the salutation\n",
    "X_test['salut'] = X_test['Name'].str.split(',',expand=True)[1].str.split('.',expand=True)[0].str.strip()\n",
    "print(\"Unique values from salut - test dataset:\\n\", X_test['salut'].unique())\n",
    "\n",
    "print (\"salut Before:\")\n",
    "print (X_test['salut'].value_counts())\n",
    "\n",
    "print (X_test.columns)\n",
    "\n",
    "X_test['salut']=X_test.apply(replace_titles, axis=1)\n",
    "\n",
    "print (\"salut After:\")\n",
    "print (X_test['salut'].value_counts())\n",
    "\n",
    "Age_salut_test = pd.crosstab(X_test.Age, X_test.salut)\n",
    "print(Age_salut_test.head(6))\n",
    "print(Age_salut_test.tail(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "add3b30e-ceda-4171-ad58-38140f50038b",
    "_uuid": "e899a2126368a63a402a685ebfa87e3eb5e71613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values for Age before imputation:  86\n",
      "Null values for Age after imputation:  0 \n",
      "\n",
      "Null values for Fare before imputation:  1\n",
      "Null values for Fare after imputation:  0 \n",
      "\n",
      "Null values for Cabin before imputation:  0\n",
      "Null values for Cabin after imputation:  0 \n",
      "\n",
      "Null values after imputation: \n",
      "PassengerId    0\n",
      "Pclass         0\n",
      "Name           0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Ticket         0\n",
      "Fare           0\n",
      "Cabin          0\n",
      "Embarked       0\n",
      "salut          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Imputing missing values - Test Data\n",
    "print (\"Null values for Age before imputation: \", X_test['Age'].isnull().sum())\n",
    "X_test['Age'] = X_test.groupby('salut').Age.transform(lambda x: x.fillna(x.mean()))\n",
    "print (\"Null values for Age after imputation: \", X_test['Age'].isnull().sum(), \"\\n\")\n",
    "\n",
    "print (\"Null values for Fare before imputation: \", X_test['Fare'].isnull().sum())\n",
    "X_test['Fare'] = X_test.groupby('Pclass').Fare.transform(lambda x: x.fillna(x.median()))\n",
    "print (\"Null values for Fare after imputation: \", X_test['Fare'].isnull().sum(), \"\\n\")\n",
    "\n",
    "print(\"Null values for Cabin before imputation: \", X_train['Cabin'].isnull().sum())\n",
    "X_test['Cabin'] = X_test['Cabin'].fillna('Null')\n",
    "print(\"Null values for Cabin after imputation: \", X_train['Cabin'].isnull().sum(), \"\\n\")\n",
    "\n",
    "# Imputing the whole dataset just in case there are any furhter missing values\n",
    "X_test = X_test.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "print(\"Null values after imputation: \")\n",
    "print(X_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null    327\n",
      "C        35\n",
      "B        18\n",
      "D        13\n",
      "E        11\n",
      "A         7\n",
      "F         6\n",
      "G         1\n",
      "Name: Deck, dtype: int64 \n",
      "\n",
      "1     253\n",
      "2      74\n",
      "3      57\n",
      "4      14\n",
      "5       7\n",
      "11      4\n",
      "7       4\n",
      "6       3\n",
      "8       2\n",
      "Name: FamilySize, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Feature Engineering\n",
    "# Deck\n",
    "X_test['Deck']=X_test['Cabin'].map(lambda x: substrings_in_string(x, cabin_list))\n",
    "print(X_test['Deck'].value_counts(), \"\\n\")\n",
    "\n",
    "# Family Size and Fare per Passenger\n",
    "X_test['FamilySize'] = X_test['SibSp'] + X_test['Parch'] + 1\n",
    "# X_test['FarePerPassenger'] = X_test['Fare']/(X_test['FamilySize'] + 1)\n",
    "\n",
    "print(X_test['FamilySize'].value_counts(), \"\\n\")\n",
    "# print()\n",
    "# print(X_test['FarePerPassenger'].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test dataset after One Hot Encoding:  (418, 466)\n",
      "                                           Name   Age  SibSp  Parch     Fare  \\\n",
      "0                              Kelly, Mr. James  34.5      0      0   7.8292   \n",
      "1              Wilkes, Mrs. James (Ellen Needs)  47.0      1      0   7.0000   \n",
      "2                     Myles, Mr. Thomas Francis  62.0      0      0   9.6875   \n",
      "3                              Wirz, Mr. Albert  27.0      0      0   8.6625   \n",
      "4  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  22.0      1      1  12.2875   \n",
      "\n",
      "   FamilySize  Pclass_1  Pclass_2  Pclass_3  Sex_female  ...  \\\n",
      "0           1         0         0         1           0  ...   \n",
      "1           2         0         0         1           1  ...   \n",
      "2           1         0         1         0           0  ...   \n",
      "3           1         0         0         1           0  ...   \n",
      "4           3         0         0         1           1  ...   \n",
      "\n",
      "   Ticket_W./C. 6608  Ticket_W.E.P. 5734  Deck_A  Deck_B  Deck_C  Deck_D  \\\n",
      "0                  0                   0       0       0       0       0   \n",
      "1                  0                   0       0       0       0       0   \n",
      "2                  0                   0       0       0       0       0   \n",
      "3                  0                   0       0       0       0       0   \n",
      "4                  0                   0       0       0       0       0   \n",
      "\n",
      "   Deck_E  Deck_F  Deck_G  Deck_Null  \n",
      "0       0       0       0          1  \n",
      "1       0       0       0          1  \n",
      "2       0       0       0          1  \n",
      "3       0       0       0          1  \n",
      "4       0       0       0          1  \n",
      "\n",
      "[5 rows x 466 columns]\n"
     ]
    }
   ],
   "source": [
    "## Converting Categorical and String features into Numeric\n",
    "\n",
    "# Drop features which are unique across respondents as they are not useful\n",
    "X_test.drop(['PassengerId'], axis=1, inplace=True)\n",
    "\n",
    "# One Hot Encoding - To convert categorical to binary data\n",
    "X_test_dummies = pd.get_dummies(X_test, columns=['Pclass', 'Sex', 'Cabin', 'Embarked', 'salut', 'Ticket', 'Deck'])\n",
    "print (\"Shape of test dataset after One Hot Encoding: \", X_test_dummies.shape)\n",
    "print (X_test_dummies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training dataset after One Hot Encoding:  (891, 855)\n",
      "Shape of test dataset after One Hot Encoding:  (418, 466)\n"
     ]
    }
   ],
   "source": [
    "print (\"Shape of training dataset after One Hot Encoding: \", X_train_dummies.shape)\n",
    "print (\"Shape of test dataset after One Hot Encoding: \", X_test_dummies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "3ef4c4c0-8a74-4685-8e78-ceea5092a7c5",
    "_uuid": "6c68eac37e1441787ae3f9234464e3bfc303b037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 855)\n",
      "(418, 855)\n",
      "Age           0\n",
      "SibSp         0\n",
      "Parch         0\n",
      "Fare          0\n",
      "FamilySize    0\n",
      "             ..\n",
      "Deck_E        0\n",
      "Deck_F        0\n",
      "Deck_G        0\n",
      "Deck_Null     0\n",
      "Deck_T        0\n",
      "Length: 855, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Align the Train and Test datset for One Hot Encoding \n",
    "X_train_final, X_test_final = X_train_dummies.align(X_test_dummies, join='left', axis=1)\n",
    "print (X_train_final.shape)\n",
    "print (X_test_final.shape)\n",
    "\n",
    "for col in (col for col in X_test_final.columns if X_test_final[col].isnull().any()):\n",
    "    X_test_final[col] = 0\n",
    "\n",
    "print(X_test_final.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final_description = X_train_final.describe().T\n",
    "X_test_final_description = X_test_final.describe().T\n",
    "\n",
    "X_train_final_description.to_csv(\"X_train_final_description.csv\")\n",
    "X_test_final_description.to_csv(\"X_test_final_description.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "X_train_standard_scaled = standard_scaler.fit_transform(X_train_final)\n",
    "# X_train_standard_scaled.mean(axis=0)\n",
    "X_test_standard_scaled = standard_scaler.fit_transform(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "682c5d7a-17c5-428a-8a5f-c6522dfec54d",
    "_uuid": "9062f190af0e65f91964fae8118adcd21c204c13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(623, 855)\n",
      "(268, 855)\n",
      "All: [61.61616162 38.38383838]\n",
      "Training: [61.63723917 38.36276083]\n",
      "Test: [61.56716418 38.43283582]\n"
     ]
    }
   ],
   "source": [
    "# split the data into train and evaluation data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X, val_X, y, val_y = train_test_split(X_train_final, y_train, train_size=0.7, test_size=0.3, random_state=123, stratify=y_train)\n",
    "\n",
    "# Applying scaled data\n",
    "X, val_X, y, val_y = train_test_split(X_train_standard_scaled, y_train, train_size=0.7, test_size=0.3, random_state=123, stratify=y_train)\n",
    "\n",
    "print (X.shape)\n",
    "print (val_X.shape)\n",
    "print('All:', np.bincount(y_train) / float(len(y_train)) * 100.0)\n",
    "print('Training:', np.bincount(y) / float(len(y)) * 100.0)\n",
    "print('Test:', np.bincount(val_y) / float(len(val_y)) * 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "60c06caf-afb3-43f5-87fe-ed1ab84f8b7e",
    "_uuid": "1a940cf7efea8fc45239ec1c0fec7df7f4a010f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False} \n",
      "\n",
      "Count for validation data - actual:  [165 103]\n",
      "Count for validation data - prediction:  [179  89] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_logistic_regression = LogisticRegression()\n",
    "model_logistic_regression.fit(X, y)\n",
    "\n",
    "print(\"Model Parameters:\")\n",
    "print(model_logistic_regression.get_params(), \"\\n\")\n",
    "\n",
    "predictions = model_logistic_regression.predict(val_X)\n",
    "predict_proba = model_logistic_regression.predict_proba(val_X)\n",
    "\n",
    "print (\"Count for validation data - actual: \", np.bincount(val_y))\n",
    "print (\"Count for validation data - prediction: \", np.bincount(predictions), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for training data - Logistic Regression:  0.9951845906902087\n",
      "Score for validation data - Logistic Regression:  0.835820895522388 \n",
      "\n",
      "Predictions:  [0 1 0 0 0 1] \n",
      "\n",
      "Prediction Probabilities:\n",
      " [[0.9809391  0.0190609 ]\n",
      " [0.00633401 0.99366599]\n",
      " [0.9860503  0.0139497 ]\n",
      " [0.99454257 0.00545743]\n",
      " [0.99868178 0.00131822]\n",
      " [0.24443106 0.75556894]]\n"
     ]
    }
   ],
   "source": [
    "# Score\n",
    "print (\"Score for training data - Logistic Regression: \", model_logistic_regression.score(X, y))\n",
    "score_val_dataset = model_logistic_regression.score(val_X, val_y)\n",
    "print (\"Score for validation data - Logistic Regression: \", score_val_dataset, \"\\n\")\n",
    "\n",
    "print(\"Predictions: \", predictions[0:6], \"\\n\")\n",
    "print(\"Prediction Probabilities:\\n\", predict_proba[0:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "9043eb8cdacd754f94a094e38fe3adf37c01e856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score for validation data - Logistic Regression:  0.7708333333333335\n",
      "f1_score (average=None) for validation data - Logistic Regression:  [0.87209302 0.77083333] \n",
      "\n",
      "precision_recall_fscore for validation data - Logistic Regression:  (array([0.83798883, 0.83146067]), array([0.90909091, 0.7184466 ]), array([0.87209302, 0.77083333]), array([165, 103], dtype=int64)) \n",
      "\n",
      "Accuracy for model Logistic Regression: 83.58\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "print (\"f1_score for validation data - Logistic Regression: \", f1_score(val_y, model_logistic_regression.predict(val_X)))\n",
    "print (\"f1_score (average=None) for validation data - Logistic Regression: \", f1_score(val_y, model_logistic_regression.predict(val_X), average=None), \"\\n\")\n",
    "\n",
    "print (\"precision_recall_fscore for validation data - Logistic Regression: \", precision_recall_fscore_support(val_y, model_logistic_regression.predict(val_X)), \"\\n\")\n",
    "\n",
    "print(\"Accuracy for model Logistic Regression: %.2f\" % (accuracy_score(val_y, model_logistic_regression.predict(val_X)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score for training data with default CV:  [0.86592179 0.8258427  0.83707865 0.84831461 0.87078652]\n",
      "0.849588851923922\n",
      "\n",
      "Score for training data with CV=3:  [0.82828283 0.84175084 0.85185185]\n",
      "0.8406285072951739\n"
     ]
    }
   ],
   "source": [
    "# Applying cross validation on the enire training data\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, ShuffleSplit\n",
    "\n",
    "# scores = cross_val_score(model_logistic_regression, X_train_final, y_train)\n",
    "scores = cross_val_score(model_logistic_regression, X_train_standard_scaled, y_train)\n",
    "print(\"\\nScore for training data with default CV: \", scores)\n",
    "print(np.mean(scores))\n",
    "\n",
    "scores = cross_val_score(model_logistic_regression, X_train_standard_scaled, y_train, cv=3)\n",
    "print(\"\\nScore for training data with CV=3: \", scores)\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "e7788a12-823e-46c4-8516-6504c630df80",
    "_uuid": "af835a1d079f47be012d4cdeaab275fc3dfea559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for training data with default CV:  [0.872      0.864      0.8        0.83064516 0.84677419]\n",
      "Mean Score:  0.8426838709677419 \n",
      "\n",
      "Score for training data with CV=3:  [0.86057692 0.82692308 0.83091787]\n",
      "Mean Score:  0.8394726247987118 \n",
      "\n",
      "Score for validation data with CV=3:  [0.74444444 0.7752809  0.74157303]\n",
      "Mean Score:  0.753766125676238\n"
     ]
    }
   ],
   "source": [
    "# Applying cross validation by splitting the training data into training data and validation data\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, ShuffleSplit\n",
    "\n",
    "scores = cross_val_score(model_logistic_regression, X, y)\n",
    "print(\"Score for training data with default CV: \", scores)\n",
    "print(\"Mean Score: \", np.mean(scores), \"\\n\")\n",
    "\n",
    "scores = cross_val_score(model_logistic_regression, X, y, cv=3)\n",
    "print(\"Score for training data with CV=3: \", scores)\n",
    "print(\"Mean Score: \", np.mean(scores), \"\\n\")\n",
    "\n",
    "scores = cross_val_score(model_logistic_regression, val_X, val_y, cv=3)\n",
    "print(\"Score for validation data with CV=3: \", scores)\n",
    "print(\"Mean Score: \", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "b2d8524c-e2f6-4d99-9ebe-5b9b0dda3b40",
    "_uuid": "369e224e3bc2e94f29c4da661999f275da399988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for validation data with StartifiedKFold, CV=5:  [0.7761194  0.7761194  0.76119403 0.71641791]\n",
      "Mean Score:  0.7574626865671642\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=4)\n",
    "scores = cross_val_score(model_logistic_regression, val_X, val_y, cv=cv)\n",
    "print(\"Score for validation data with StartifiedKFold, CV=5: \", scores)\n",
    "print(\"Mean Score: \", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "562daf63-8a3a-491a-b400-a1376c164ae5",
    "_uuid": "4473beab4bc8c184145834cee2910590d387fe27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] ............................. C=0.001, score=0.718, total=   0.0s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] ............................. C=0.001, score=0.712, total=   0.0s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] ............................. C=0.001, score=0.699, total=   0.0s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] ............................. C=0.001, score=0.742, total=   0.0s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] .............................. C=0.01, score=0.846, total=   0.0s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] .............................. C=0.01, score=0.840, total=   0.0s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] .............................. C=0.01, score=0.859, total=   0.0s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] .............................. C=0.01, score=0.819, total=   0.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................... C=0.1, score=0.865, total=   0.0s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................... C=0.1, score=0.814, total=   0.0s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................... C=0.1, score=0.840, total=   0.0s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............................... C=0.1, score=0.832, total=   0.0s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] ................................. C=1, score=0.859, total=   0.1s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] ................................. C=1, score=0.827, total=   0.1s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] ................................. C=1, score=0.840, total=   0.1s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] ................................. C=1, score=0.832, total=   0.1s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ................................ C=10, score=0.865, total=   0.1s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ................................ C=10, score=0.821, total=   0.2s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ................................ C=10, score=0.833, total=   0.2s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ................................ C=10, score=0.832, total=   0.1s\n",
      "Parameters:  <bound method BaseEstimator.get_params of GridSearchCV(cv=StratifiedKFold(n_splits=4, random_state=None, shuffle=False),\n",
      "             error_score=nan,\n",
      "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
      "                                          fit_intercept=True,\n",
      "                                          intercept_scaling=1, l1_ratio=None,\n",
      "                                          max_iter=200, multi_class='auto',\n",
      "                                          n_jobs=None, penalty='l2',\n",
      "                                          random_state=None, solver='lbfgs',\n",
      "                                          tol=0.0001, verbose=0,\n",
      "                                          warm_start=False),\n",
      "             iid='deprecated', n_jobs=None,\n",
      "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10]},\n",
      "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
      "             scoring=None, verbose=3)>\n",
      "\n",
      "GridSearchCV best score - Logistic Regression:  0.841056658395368\n",
      "\n",
      "GridSearchCV best params - Logistic Regression:  {'C': 0.01}\n",
      "\n",
      "GridSearchCV best estimator - Logistic Regression:  LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "\n",
      "GridSearchCV Score for validation data - Logistic Regression:  0.835820895522388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    1.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# param_grid = {'C': [0.001, 0.01, 0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1, 1]}\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "model_logistic_regression_grid = GridSearchCV(LogisticRegression(max_iter=200), param_grid=param_grid, cv=cv, verbose=3)\n",
    "model_logistic_regression_grid.fit(X, y)\n",
    "\n",
    "results_grid = model_logistic_regression_grid.cv_results_\n",
    "\n",
    "print (\"Parameters: \", model_logistic_regression_grid.get_params)\n",
    "\n",
    "print(\"\\nGridSearchCV best score - Logistic Regression: \", model_logistic_regression_grid.best_score_)\n",
    "print(\"\\nGridSearchCV best params - Logistic Regression: \", model_logistic_regression_grid.best_params_)\n",
    "print(\"\\nGridSearchCV best estimator - Logistic Regression: \", model_logistic_regression_grid.best_estimator_)\n",
    "\n",
    "print (\"\\nGridSearchCV Score for validation data - Logistic Regression: \", model_logistic_regression_grid.score(val_X, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01749367, 0.01774353, 0.04297793, 0.06271452, 0.14492041]),\n",
       " 'std_fit_time': array([0.0041497 , 0.00147642, 0.00463353, 0.00395804, 0.03478024]),\n",
       " 'mean_score_time': array([0.00125062, 0.00074977, 0.00074917, 0.00124776, 0.00124878]),\n",
       " 'std_score_time': array([0.00043519, 0.00043288, 0.00043253, 0.0008278 , 0.00043295]),\n",
       " 'param_C': masked_array(data=[0.001, 0.01, 0.1, 1, 10],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.001}, {'C': 0.01}, {'C': 0.1}, {'C': 1}, {'C': 10}],\n",
       " 'split0_test_score': array([0.71794872, 0.84615385, 0.86538462, 0.85897436, 0.86538462]),\n",
       " 'split1_test_score': array([0.71153846, 0.83974359, 0.81410256, 0.82692308, 0.82051282]),\n",
       " 'split2_test_score': array([0.69871795, 0.85897436, 0.83974359, 0.83974359, 0.83333333]),\n",
       " 'split3_test_score': array([0.74193548, 0.81935484, 0.83225806, 0.83225806, 0.83225806]),\n",
       " 'mean_test_score': array([0.71753515, 0.84105666, 0.83787221, 0.83947477, 0.83787221]),\n",
       " 'std_test_score': array([0.01569709, 0.01431536, 0.0184184 , 0.01214426, 0.01666134]),\n",
       " 'rank_test_score': array([5, 1, 3, 2, 3])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:\n",
      "{'cv': StratifiedKFold(n_splits=4, random_state=None, shuffle=False), 'error_score': nan, 'estimator__C': 1.0, 'estimator__class_weight': None, 'estimator__dual': False, 'estimator__fit_intercept': True, 'estimator__intercept_scaling': 1, 'estimator__l1_ratio': None, 'estimator__max_iter': 200, 'estimator__multi_class': 'auto', 'estimator__n_jobs': None, 'estimator__penalty': 'l2', 'estimator__random_state': None, 'estimator__solver': 'lbfgs', 'estimator__tol': 0.0001, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), 'iid': 'deprecated', 'n_jobs': None, 'param_grid': {'C': [0.001, 0.01, 0.1, 1, 10]}, 'pre_dispatch': '2*n_jobs', 'refit': True, 'return_train_score': False, 'scoring': None, 'verbose': 3} \n",
      "\n",
      "Count for validation data - actual:  [165 103]\n",
      "Count for validation data - prediction-grid:  [187  81] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Parameters:\")\n",
    "print(model_logistic_regression_grid.get_params(), \"\\n\")\n",
    "\n",
    "predictions_grid = model_logistic_regression_grid.predict(val_X)\n",
    "predict_proba_grid = model_logistic_regression_grid.predict_proba(val_X)\n",
    "\n",
    "print (\"Count for validation data - actual: \", np.bincount(val_y))\n",
    "print (\"Count for validation data - prediction-grid: \", np.bincount(predictions_grid), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for training data - Logistic Regression:  0.9935794542536116\n",
      "Score for validation data - Logistic Regression:  0.835820895522388 \n",
      "\n",
      "Predictions grid:  [0 1 0 0 0 1] \n",
      "\n",
      "Prediction Probabilities grid:\n",
      " [[0.80778833 0.19221167]\n",
      " [0.19279573 0.80720427]\n",
      " [0.81923486 0.18076514]\n",
      " [0.87460092 0.12539908]\n",
      " [0.90283192 0.09716808]\n",
      " [0.4675212  0.5324788 ]]\n"
     ]
    }
   ],
   "source": [
    "# Score\n",
    "print (\"Score for training data - Logistic Regression: \", model_logistic_regression_grid.score(X, y))\n",
    "score_grid_val_dataset = model_logistic_regression_grid.score(val_X, val_y)\n",
    "print (\"Score for validation data - Logistic Regression: \", score_grid_val_dataset, \"\\n\")\n",
    "\n",
    "print(\"Predictions grid: \", predictions_grid[0:6], \"\\n\")\n",
    "print(\"Prediction Probabilities grid:\\n\", predict_proba_grid[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......... C=0.001, Accuracy=0.718, F1 Score=0.476, total=   0.0s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......... C=0.001, Accuracy=0.712, F1 Score=0.400, total=   0.0s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......... C=0.001, Accuracy=0.699, F1 Score=0.373, total=   0.0s\n",
      "[CV] C=0.001 .........................................................\n",
      "[CV] .......... C=0.001, Accuracy=0.742, F1 Score=0.500, total=   0.0s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........... C=0.01, Accuracy=0.846, F1 Score=0.782, total=   0.0s\n",
      "[CV] C=0.01 ..........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........... C=0.01, Accuracy=0.840, F1 Score=0.766, total=   0.0s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........... C=0.01, Accuracy=0.859, F1 Score=0.800, total=   0.0s\n",
      "[CV] C=0.01 ..........................................................\n",
      "[CV] ........... C=0.01, Accuracy=0.819, F1 Score=0.750, total=   0.0s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............ C=0.1, Accuracy=0.865, F1 Score=0.826, total=   0.0s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............ C=0.1, Accuracy=0.814, F1 Score=0.729, total=   0.0s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............ C=0.1, Accuracy=0.840, F1 Score=0.790, total=   0.0s\n",
      "[CV] C=0.1 ...........................................................\n",
      "[CV] ............ C=0.1, Accuracy=0.832, F1 Score=0.783, total=   0.0s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] .............. C=1, Accuracy=0.859, F1 Score=0.820, total=   0.0s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] .............. C=1, Accuracy=0.827, F1 Score=0.752, total=   0.1s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] .............. C=1, Accuracy=0.840, F1 Score=0.797, total=   0.1s\n",
      "[CV] C=1 .............................................................\n",
      "[CV] .............. C=1, Accuracy=0.832, F1 Score=0.783, total=   0.1s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ............. C=10, Accuracy=0.865, F1 Score=0.826, total=   0.2s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ............. C=10, Accuracy=0.821, F1 Score=0.745, total=   0.1s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ............. C=10, Accuracy=0.833, F1 Score=0.790, total=   0.2s\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ............. C=10, Accuracy=0.832, F1 Score=0.783, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    1.3s finished\n"
     ]
    }
   ],
   "source": [
    "scoring = {'Accuracy': 'accuracy', 'F1 Score': 'f1'}\n",
    "model_logistic_regression_grid_with_scoring = GridSearchCV(LogisticRegression(max_iter=200), param_grid=param_grid, cv=cv, verbose=3, scoring = scoring, refit=False)\n",
    "model_logistic_regression_grid_with_scoring.fit(X, y)\n",
    "results_grid1 = model_logistic_regression_grid_with_scoring.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(sklearn.metrics.SCORERS.keys())\n",
    "results_grid_with_scoring = model_logistic_regression_grid_with_scoring.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.0267365 , 0.02698654, 0.03848445, 0.069969  , 0.15291595]),\n",
       " 'std_fit_time': array([0.00311364, 0.00254804, 0.00086759, 0.0211019 , 0.03262181]),\n",
       " 'mean_score_time': array([0.00274664, 0.00374728, 0.002244  , 0.00274181, 0.00249678]),\n",
       " 'std_score_time': array([0.00082786, 0.00148003, 0.00043627, 0.0008349 , 0.00086675]),\n",
       " 'param_C': masked_array(data=[0.001, 0.01, 0.1, 1, 10],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.001}, {'C': 0.01}, {'C': 0.1}, {'C': 1}, {'C': 10}],\n",
       " 'split0_test_Accuracy': array([0.71794872, 0.84615385, 0.86538462, 0.85897436, 0.86538462]),\n",
       " 'split1_test_Accuracy': array([0.71153846, 0.83974359, 0.81410256, 0.82692308, 0.82051282]),\n",
       " 'split2_test_Accuracy': array([0.69871795, 0.85897436, 0.83974359, 0.83974359, 0.83333333]),\n",
       " 'split3_test_Accuracy': array([0.74193548, 0.81935484, 0.83225806, 0.83225806, 0.83225806]),\n",
       " 'mean_test_Accuracy': array([0.71753515, 0.84105666, 0.83787221, 0.83947477, 0.83787221]),\n",
       " 'std_test_Accuracy': array([0.01569709, 0.01431536, 0.0184184 , 0.01214426, 0.01666134]),\n",
       " 'rank_test_Accuracy': array([5, 1, 3, 2, 3]),\n",
       " 'split0_test_F1 Score': array([0.47619048, 0.78181818, 0.82644628, 0.81967213, 0.82644628]),\n",
       " 'split1_test_F1 Score': array([0.4       , 0.76635514, 0.72897196, 0.75229358, 0.74545455]),\n",
       " 'split2_test_F1 Score': array([0.37333333, 0.8       , 0.78991597, 0.79674797, 0.79032258]),\n",
       " 'split3_test_F1 Score': array([0.5       , 0.75      , 0.78333333, 0.78333333, 0.78333333]),\n",
       " 'mean_test_F1 Score': array([0.43738095, 0.77454333, 0.78216689, 0.78801175, 0.78638919]),\n",
       " 'std_test_F1 Score': array([0.05226557, 0.01850937, 0.03482734, 0.02437405, 0.02874468]),\n",
       " 'rank_test_F1 Score': array([5, 4, 3, 1, 2])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_grid_with_scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
